{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5k-8v19h9zU3",
    "outputId": "6d862b9a-2341-4d2d-d151-0988d8d0a1a9"
   },
   "outputs": [],
   "source": [
    "FACE_DATA_PATH=r'G:\\forgithub\\SiameseNet\\datasets\\Extracted Faces\\Extracted Faces'\n",
    "EXTRACTED_FACES_PATH =r'G:\\forgithub\\SiameseNet\\datasets\\Face Data\\Face Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jO84Muiv93QE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.applications import EfficientNetV2B3\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9NR26AgR-Vhb"
   },
   "outputs": [],
   "source": [
    "def explore_folder(folder_path):\n",
    "    print(f'Exploring {os.path.basename(folder_path)}')\n",
    "    image_shapes = []\n",
    "    num_images = 0\n",
    "    num_people = 0\n",
    "    for folder_name in os.listdir(folder_path):\n",
    "        subfolder_path = os.path.join(folder_path, folder_name)\n",
    "        for image_name in os.listdir(subfolder_path):\n",
    "            image_path = os.path.join(subfolder_path, image_name)\n",
    "            image = cv2.imread(image_path)\n",
    "            image_shapes.append(image.shape)\n",
    "            num_images += 1\n",
    "        num_people +=1\n",
    "\n",
    "    print(f'Unique image shapes in: {set(image_shapes)}')\n",
    "    print(f\"Total number of images: {num_images}\")\n",
    "    print(f\"Total number of people: {num_people}\")\n",
    "    return image_shapes, num_images, num_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring Extracted Faces\n",
      "Unique image shapes in: {(128, 128, 3)}\n",
      "Total number of images: 6107\n",
      "Total number of people: 1324\n"
     ]
    }
   ],
   "source": [
    "explore_folder(FACE_DATA_PATH);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "08Wl7eTz-f10"
   },
   "outputs": [],
   "source": [
    "DATASET = 'images/output_dataset'\n",
    "if os.path.exists(DATASET):\n",
    "    shutil.rmtree(DATASET)\n",
    "os.makedirs(DATASET)\n",
    "\n",
    "def copy_to_output_dataset(input_path, output_path):\n",
    "    for person_folder in os.listdir(input_path):\n",
    "        person_folder_path = os.path.join(input_path, person_folder)\n",
    "        if os.path.isdir(person_folder_path):\n",
    "            output_person_folder = os.path.join(output_path, person_folder)\n",
    "            if not os.path.exists(output_person_folder):\n",
    "                os.makedirs(output_person_folder)\n",
    "\n",
    "            for image_file in os.listdir(person_folder_path):\n",
    "                if image_file.endswith('.jpg'):\n",
    "                    src_image_path = os.path.join(person_folder_path, image_file)\n",
    "                    dst_image_path = os.path.join(output_person_folder, image_file)\n",
    "                    if os.path.exists(dst_image_path):\n",
    "                        base, ext = os.path.splitext(dst_image_path)\n",
    "                        dst_image_path = f\"{base}_1{ext}\"\n",
    "                    shutil.copy(src_image_path, dst_image_path)\n",
    "\n",
    "copy_to_output_dataset(FACE_DATA_PATH, DATASET)\n",
    "copy_to_output_dataset(EXTRACTED_FACES_PATH, DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9lYLKDYU-nkU"
   },
   "outputs": [],
   "source": [
    "def triplets(folder_paths, max_triplets=7):\n",
    "    anchor_images = []\n",
    "    positive_images = []\n",
    "    negative_images = []\n",
    "\n",
    "    for person_folder in folder_paths:\n",
    "        images = [os.path.join(person_folder, img)\n",
    "                  for img in os.listdir(person_folder)]\n",
    "        num_images = len(images)\n",
    "\n",
    "        if num_images < 2:\n",
    "            continue\n",
    "\n",
    "        random.shuffle(images)\n",
    "\n",
    "        for _ in range(max(num_images-1, max_triplets)):\n",
    "            anchor_image = random.choice(images)\n",
    "\n",
    "            positive_image = random.choice([x for x in images\n",
    "                                            if x != anchor_image])\n",
    "\n",
    "            negative_folder = random.choice([x for x in folder_paths\n",
    "                                             if x != person_folder])\n",
    "\n",
    "            negative_image = random.choice([os.path.join(negative_folder, img)\n",
    "                                            for img in os.listdir(negative_folder)])\n",
    "\n",
    "            anchor_images.append(anchor_image)\n",
    "            positive_images.append(positive_image)\n",
    "            negative_images.append(negative_image)\n",
    "\n",
    "    return anchor_images, positive_images, negative_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UY2wmi6F-sr7"
   },
   "outputs": [],
   "source": [
    "person_folders = [os.path.join(DATASET, folder_name)\n",
    "                  for folder_name in os.listdir(DATASET)]\n",
    "\n",
    "anchors, positives, negatives = triplets(person_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zVlgZnBwl55",
    "outputId": "5a249890-0554-4675-e5d8-88f0ea9e0eb2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17318, 17318, 17318)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anchors), len(positives), len(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1_lWq5qQ-ufy"
   },
   "outputs": [],
   "source": [
    "def split_triplets(anchors,\n",
    "                   positives,\n",
    "                   negatives,\n",
    "                   validation_split=0.2):\n",
    "\n",
    "    triplets = list(zip(anchors, positives, negatives))\n",
    "\n",
    "    train_triplets, val_triplets = train_test_split(triplets,\n",
    "                                                    test_size=validation_split,\n",
    "                                                    random_state=42)\n",
    "\n",
    "    return train_triplets, val_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYtCb2Bb-wxz",
    "outputId": "4cd6728c-e468-4490-9cd9-837822faef69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13854, 3464)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_triplets, val_triplets = split_triplets(anchors,\n",
    "                                              positives,\n",
    "                                              negatives)\n",
    "len(train_triplets), len(val_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0xZNHRRU-yTy"
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path, expand_dims=False):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Image not found or could not be loaded: {image_path}\")\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (128, 128))\n",
    "    if expand_dims:\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "    return image\n",
    "\n",
    "\n",
    "def batch_generator(triplets, batch_size=32, augment=True):\n",
    "    total_triplets = len(triplets)\n",
    "    random_indices = list(range(total_triplets))\n",
    "    random.shuffle(random_indices)\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.05,\n",
    "        height_shift_range=0.05,\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=0.2\n",
    "    )\n",
    "\n",
    "    for i in range(0, total_triplets, batch_size):\n",
    "        batch_indices = random_indices[i:i + batch_size]\n",
    "        if len(batch_indices) < batch_size:\n",
    "            continue  # Skip the last incomplete batch\n",
    "\n",
    "        batch_triplets = [triplets[j] for j in batch_indices]\n",
    "        anchor_batch, positive_batch, negative_batch = [], [], []\n",
    "\n",
    "        for triplet in batch_triplets:\n",
    "            anchor, positive, negative = triplet\n",
    "\n",
    "            anchor_image = load_and_preprocess_image(anchor)\n",
    "            positive_image = load_and_preprocess_image(positive)\n",
    "            negative_image = load_and_preprocess_image(negative)\n",
    "\n",
    "            if augment:\n",
    "                anchor_image = datagen.random_transform(anchor_image)\n",
    "                positive_image = datagen.random_transform(positive_image)\n",
    "                negative_image = datagen.random_transform(negative_image)\n",
    "\n",
    "            anchor_batch.append(anchor_image)\n",
    "            positive_batch.append(positive_image)\n",
    "            negative_batch.append(negative_image)\n",
    "\n",
    "        yield (np.array(anchor_batch), np.array(positive_batch), np.array(negative_batch)), np.ones((len(batch_indices), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n",
      "Memory growth set for GPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(\"GPU is available.\")\n",
    "    # Optionally set memory growth to avoid OOM errors\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Memory growth set for GPU.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "TmqHSY1k-4kk"
   },
   "outputs": [],
   "source": [
    "# Embedding Model\n",
    "class EmbeddingModel(Model):\n",
    "    def __init__(self, embedding_size=128, dropout_rate=0.5):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "\n",
    "        # Use EfficientNetV2B3 as the backbone for feature extraction\n",
    "        self.feature_extractor = EfficientNetV2B3(input_shape=(128, 128, 3),\n",
    "                                                  include_top=False,\n",
    "                                                  weights='imagenet')\n",
    "        \n",
    "        # Freeze the layers of EfficientNetV2B3\n",
    "        self.feature_extractor.trainable = False\n",
    "\n",
    "        # Add flattening layer for converting 2D feature maps to 1D vectors\n",
    "        self.flatten = Flatten(name=\"flatten_layer\")\n",
    "\n",
    "        self.fc1 = Dense(512, activation='relu', name=\"fc1_dense\")\n",
    "        self.batchnorm1 = BatchNormalization(name=\"batchnorm1\")\n",
    "        self.dropout1 = Dropout(dropout_rate, name=\"dropout1\")\n",
    "\n",
    "        self.fc2 = Dense(256, activation='relu', name=\"fc2_dense\")\n",
    "        self.batchnorm2 = BatchNormalization(name=\"batchnorm2\")\n",
    "        self.dropout2 = Dropout(dropout_rate, name=\"dropout2\")\n",
    "\n",
    "        # Output embedding layer with custom embedding size\n",
    "        self.embedding_output = Dense(embedding_size, name=\"embedding_output\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Pass inputs through feature extractor (EfficientNetV2B3)\n",
    "        x = self.feature_extractor(inputs)\n",
    "        \n",
    "        # Flatten the output feature map\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Fully connected layers with batch normalization and dropout\n",
    "        x = self.fc1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Return the final embedding\n",
    "        return self.embedding_output(x)\n",
    "\n",
    "# Custom Triplet Loss Layer\n",
    "class TripletLayer(Layer):\n",
    "    def __init__(self, margin=1.0, **kwargs):\n",
    "        super(TripletLayer, self).__init__(**kwargs)\n",
    "        self.margin = margin\n",
    "\n",
    "    def call(self, anchor_embeddings, positive_embeddings, negative_embeddings):\n",
    "        # Calculate squared Euclidean distance between anchor and positive/negative samples\n",
    "        positive_distance = tf.reduce_sum(tf.square(anchor_embeddings - positive_embeddings), axis=1)\n",
    "        negative_distance = tf.reduce_sum(tf.square(anchor_embeddings - negative_embeddings), axis=1)\n",
    "\n",
    "        # Triplet loss: max(positive_distance - negative_distance + margin, 0)\n",
    "        triplet_loss = tf.maximum(positive_distance - negative_distance + self.margin, 0.0)\n",
    "\n",
    "        return tf.reduce_mean(triplet_loss)\n",
    "\n",
    "# Triplet Siamese Model\n",
    "class TripletSiameseModel(Model):\n",
    "    def __init__(self, embedding_size=128, margin=1.0, dropout_rate=0.5):\n",
    "        super(TripletSiameseModel, self).__init__()\n",
    "        \n",
    "        # Base model for generating embeddings\n",
    "        self.embedding_model = EmbeddingModel(embedding_size=embedding_size, dropout_rate=dropout_rate)\n",
    "        \n",
    "        # Triplet loss layer with the specified margin\n",
    "        self.triplet_loss_layer = TripletLayer(margin=margin, name=\"triplet_loss_layer\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Unpack the inputs into anchor, positive, and negative examples\n",
    "        anchor_input, positive_input, negative_input = inputs\n",
    "\n",
    "        # Generate embeddings for anchor, positive, and negative examples\n",
    "        anchor_embeddings = self.embedding_model(anchor_input)\n",
    "        positive_embeddings = self.embedding_model(positive_input)\n",
    "        negative_embeddings = self.embedding_model(negative_input)\n",
    "\n",
    "        # Return the triplet loss\n",
    "        return self.triplet_loss_layer(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
    "\n",
    "    # Build the model graph for easy visualization\n",
    "    def build_graph(self):\n",
    "        anchor_input = Input(shape=(128, 128, 3), name=\"anchor_input\")\n",
    "        positive_input = Input(shape=(128, 128, 3), name=\"positive_input\")\n",
    "        negative_input = Input(shape=(128, 128, 3), name=\"negative_input\")\n",
    "        \n",
    "        # Create the model\n",
    "        return Model(inputs=[anchor_input, positive_input, negative_input], \n",
    "                     outputs=self.call([anchor_input, positive_input, negative_input]))\n",
    "\n",
    "# Identity Loss\n",
    "def identity_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Stdxy6mq_waS",
    "outputId": "83432c07-d39b-43ae-f2fe-75ff6a21ccd1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 128])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_test = np.random.rand(1, 128, 128, 3)\n",
    "model = baseMobileModel()\n",
    "model(input_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6PUYRlTAGFD",
    "outputId": "f28188af-779a-4684-e80c-467cef94d270"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss: 7.421262\n"
     ]
    }
   ],
   "source": [
    "triplet_layer = TripletLayer(margin=1)\n",
    "anchors = tf.random.normal((5, 128))\n",
    "positives = tf.random.normal((5, 128))\n",
    "negatives = tf.random.normal((5, 128))\n",
    "loss = triplet_layer(anchors, positives, negatives)\n",
    "\n",
    "print(\"Triplet Loss:\", loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "8Muug35DCZcF"
   },
   "outputs": [],
   "source": [
    "loss_tracker = identity_loss\n",
    "optimizer = Adam(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "QfBejPgFNLZT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/109 [05:05<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "siamese_net = TripletSiameseModel(embedding_size=256, margin=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "L1je7IeGrvLD"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, batch_generator, model, loss_fun, optimizer):\n",
    "        \"\"\"\n",
    "        Initializes the Trainer class with a model, loss function, and optimizer.\n",
    "        Args:\n",
    "            model: Siamese model.\n",
    "            loss_fun: The loss function to optimize.\n",
    "            optimizer: The optimizer to apply during training.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.loss_fun = loss_fun\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_generator = batch_generator\n",
    "\n",
    "    def __call__(self, train_triplets, val_triplets, epochs, batch_size):\n",
    "        \"\"\"\n",
    "        Executes the training and validation loops for the specified epochs and batch size.\n",
    "        Args:\n",
    "            train_triplets: Training data in triplet format (anchor, positive, negative).\n",
    "            val_triplets: Validation data in triplet format (anchor, positive, negative).\n",
    "            epochs: Number of epochs to train the model.\n",
    "            batch_size: Size of each batch during training and validation.\n",
    "        Returns:\n",
    "            A tuple containing lists of training and validation losses for each epoch.\n",
    "        \"\"\"\n",
    "        train_epochs_losses = []\n",
    "        val_epochs_losses = []\n",
    "\n",
    "        # Compute steps per epoch\n",
    "        train_steps_per_epoch = math.ceil(len(train_triplets) / batch_size)\n",
    "        val_steps_per_epoch = math.ceil(len(val_triplets) / batch_size)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Initialize generators for training and validation\n",
    "            train_generator = self.batch_generator(train_triplets, batch_size=batch_size, augment=True)\n",
    "            val_generator = self.batch_generator(val_triplets, batch_size=batch_size, augment=False)\n",
    "            print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "            # Training step\n",
    "            train_losses = self.train_step_for_one_epoch(train_generator, train_steps_per_epoch)\n",
    "            avg_train_loss = np.mean(train_losses)\n",
    "            train_epochs_losses.append(avg_train_loss)\n",
    "            print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "            # Validation step\n",
    "            val_losses = self.perform_validation(val_generator, val_steps_per_epoch)\n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            val_epochs_losses.append(avg_val_loss)\n",
    "            print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        return train_epochs_losses, val_epochs_losses\n",
    "\n",
    "    @tf.function\n",
    "    def apply_gradients(self, anchors, positives, negatives, labels):\n",
    "        \"\"\"\n",
    "        Applies gradients and updates the model weights using the optimizer.\n",
    "        Args:\n",
    "            anchors: Anchor images (input).\n",
    "            positives: Positive images (input).\n",
    "            negatives: Negative images (input).\n",
    "            labels: Labels for the triplet loss.\n",
    "        Returns:\n",
    "            The predicted values and the computed loss for the batch.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self.model([anchors, positives, negatives], training=True)\n",
    "            loss = self.loss_fun(labels, y_pred)\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        return y_pred, loss\n",
    "\n",
    "    def train_step_for_one_epoch(self, train_generator, train_steps_per_epoch):\n",
    "        \"\"\"\n",
    "        Performs one epoch of training across all batches.\n",
    "        Args:\n",
    "            train_generator: Generator that yields batches of training data.\n",
    "            train_steps_per_epoch: Number of training steps in one epoch.\n",
    "        Returns:\n",
    "            A list of losses for each batch in the epoch.\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        pbar = tqdm(total=train_steps_per_epoch, position=0, leave=True)\n",
    "        for step, ((anchors, positives, negatives), labels) in enumerate(train_generator):\n",
    "            # Convert to TensorFlow tensors if not already\n",
    "            anchors, positives, negatives, labels = map(tf.convert_to_tensor, (anchors, positives, negatives, labels))\n",
    "            \n",
    "            y_pred, loss = self.apply_gradients(anchors, positives, negatives, labels)\n",
    "            losses.append(loss)  # Convert Tensor loss to a numpy float\n",
    "\n",
    "            pbar.set_description(f\"Training Loss: {float(loss):.4f}\")\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "        return losses\n",
    "        \n",
    "    \n",
    "    def perform_validation(self, val_generator, val_steps_per_epoch):\n",
    "        \"\"\"\n",
    "        Performs validation across all batches.\n",
    "        Args:\n",
    "            val_generator: Generator that yields batches of validation data.\n",
    "            val_steps_per_epoch: Number of validation steps in one epoch.\n",
    "        Returns:\n",
    "            A list of losses for each batch in the validation epoch.\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        pbar = tqdm(total=val_steps_per_epoch, position=0, leave=True)\n",
    "        for step, ((anchors, positives, negatives), labels) in enumerate(val_generator):\n",
    "            # Convert to TensorFlow tensors if not already\n",
    "            anchors, positives, negatives, labels = map(tf.convert_to_tensor, (anchors, positives, negatives, labels))\n",
    "\n",
    "            y_pred = self.model([anchors, positives, negatives], training=False)\n",
    "            loss = self.loss_fun(labels, y_pred)\n",
    "            \n",
    "            # Convert the loss tensor to a numpy float for logging and storing\n",
    "            loss_value = loss.numpy()  # Ensure the tensor is converted to a NumPy value\n",
    "            losses.append(loss_value)\n",
    "\n",
    "            pbar.set_description(f\"Validation Loss: {loss_value:.4f}\")\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "T4hQpmQDVyby",
    "outputId": "1e7d0e00-1b32-416e-e56a-f96bb59da752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 7.1126:  12%|█▏        | 25/217 [1:03:48<8:10:01, 153.13s/it]\n",
      "Training Loss: 1.3496: 100%|█████████▉| 216/217 [04:41<00:01,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 2.2107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6112:  98%|█████████▊| 54/55 [00:46<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 1.0824\n",
      "\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.8720: 100%|█████████▉| 216/217 [04:13<00:01,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.7828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6292:  98%|█████████▊| 54/55 [00:50<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.7454\n",
      "\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.2179: 100%|█████████▉| 216/217 [04:16<00:01,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.5653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5879:  98%|█████████▊| 54/55 [00:49<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.7621\n",
      "\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.0995: 100%|█████████▉| 216/217 [04:34<00:01,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.2540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8493:  98%|█████████▊| 54/55 [00:53<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.6018\n",
      "\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6707: 100%|█████████▉| 216/217 [04:17<00:01,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 1.1103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3095:  98%|█████████▊| 54/55 [00:46<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.4617\n",
      "\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.1718: 100%|█████████▉| 216/217 [04:17<00:01,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.9581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3479:  98%|█████████▊| 54/55 [00:47<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.4609\n",
      "\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.9809: 100%|█████████▉| 216/217 [04:14<00:01,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.7810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2533:  98%|█████████▊| 54/55 [00:45<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.3066\n",
      "\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7170: 100%|█████████▉| 216/217 [04:13<00:01,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.6739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3633:  98%|█████████▊| 54/55 [00:45<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.2757\n",
      "\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5073: 100%|█████████▉| 216/217 [04:13<00:01,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.5892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1829:  98%|█████████▊| 54/55 [00:45<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.2731\n",
      "\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4990: 100%|█████████▉| 216/217 [04:16<00:01,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.5135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0685:  98%|█████████▊| 54/55 [00:49<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.2549\n",
      "\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5273: 100%|█████████▉| 216/217 [04:16<00:01,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.4537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2317:  98%|█████████▊| 54/55 [00:49<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.2238\n",
      "\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2757: 100%|█████████▉| 216/217 [04:09<00:01,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.4169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1662:  98%|█████████▊| 54/55 [00:49<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.2270\n",
      "\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4448: 100%|█████████▉| 216/217 [04:11<00:01,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.3764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2130:  98%|█████████▊| 54/55 [00:48<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.1893\n",
      "\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2792: 100%|█████████▉| 216/217 [04:02<00:01,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.3430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1678:  98%|█████████▊| 54/55 [00:50<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.1871\n",
      "\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3169: 100%|█████████▉| 216/217 [04:05<00:01,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.3206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1622:  98%|█████████▊| 54/55 [00:47<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.1664\n",
      "\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4213: 100%|█████████▉| 216/217 [04:03<00:01,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.3028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2293:  98%|█████████▊| 54/55 [00:46<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.1601\n",
      "\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4077: 100%|█████████▉| 216/217 [03:56<00:01,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1586:  98%|█████████▊| 54/55 [00:46<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.1558\n",
      "\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2876: 100%|█████████▉| 216/217 [03:56<00:01,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1268:  98%|█████████▊| 54/55 [00:50<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.1596\n",
      "\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2971: 100%|█████████▉| 216/217 [04:07<00:01,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1399:  98%|█████████▊| 54/55 [00:47<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.1451\n",
      "\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2208: 100%|█████████▉| 216/217 [04:01<00:01,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1684:  98%|█████████▊| 54/55 [00:46<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.1710\n",
      "\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2060: 100%|█████████▉| 216/217 [04:07<00:01,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2428:  98%|█████████▊| 54/55 [00:50<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.1670\n",
      "\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2891: 100%|█████████▉| 216/217 [04:01<00:01,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2271:  98%|█████████▊| 54/55 [00:45<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.1621\n",
      "\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3557: 100%|█████████▉| 216/217 [03:56<00:01,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1919:  98%|█████████▊| 54/55 [00:45<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.1515\n",
      "\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1986: 100%|█████████▉| 216/217 [03:56<00:01,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1346:  98%|█████████▊| 54/55 [00:45<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.1496\n",
      "\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2320: 100%|█████████▉| 216/217 [04:18<00:01,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.2451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1281:  98%|█████████▊| 54/55 [00:54<00:01,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.1469\n",
      "\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3321:  15%|█▌        | 33/217 [00:40<03:37,  1.18s/it]"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    batch_generator,\n",
    "    siamese_net,\n",
    "    identity_loss,\n",
    "    optimizer\n",
    "  )\n",
    "\n",
    "his_losses = trainer(\n",
    "    train_triplets,\n",
    "    val_triplets,\n",
    "    epochs=30,\n",
    "    batch_size=64\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to read, resize, normalize, and reshape an image\n",
    "def process_image(image_path):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Resize the image to 128x128\n",
    "    image_resized = cv2.resize(image, (128, 128))\n",
    "    \n",
    "    # Normalize the image to [0, 1]\n",
    "    image_normalized = image_resized / 255.0\n",
    "    \n",
    "    # Reshape the image to (1, 128, 128, 3)\n",
    "    image_reshaped = image_normalized[np.newaxis, ...]  # Add new axis\n",
    "\n",
    "    return image_reshaped\n",
    "\n",
    "# List of image paths\n",
    "image_paths = ['/content/hos1.PNG', '/content/hos2.PNG', '/content/hos3.PNG', '/content/allam.PNG', '/content/donia.PNG']\n",
    "\n",
    "# Process the images\n",
    "processed_images = [process_image(path) for path in image_paths]\n",
    "\n",
    "# Show the processed images\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i, img in enumerate(processed_images):\n",
    "    plt.subplot(1, len(processed_images), i + 1)\n",
    "    plt.imshow(img[0])  # Remove the first dimension for display\n",
    "    plt.axis('off')     # Hide axis\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
